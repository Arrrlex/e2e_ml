{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Steel plate faults multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot create a consistent method resolution\norder (MRO) for bases ClassificationModels, PreprocessingBluePrint",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-3-ab71e66f93b7>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# load libraries\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0me2eml\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclassification\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mclassification_blueprints\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mcb\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0me2eml\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfull_processing\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpostprocessing\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0msave_to_production\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mload_for_production\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0me2eml\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtest\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclassification_blueprints_test\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0msteel_fault_multiclass_data\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmetrics\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mmatthews_corrcoef\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/IdeaProjects/e2e_ml/e2eml/classification/classification_blueprints.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m \u001B[0;32mclass\u001B[0m \u001B[0mClassificationBluePrint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mClassificationModels\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mPreprocessingBluePrint\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      7\u001B[0m     \"\"\"\n\u001B[1;32m      8\u001B[0m     \u001B[0mRuns\u001B[0m \u001B[0ma\u001B[0m \u001B[0mblue\u001B[0m \u001B[0mprint\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpreprocessing\u001B[0m \u001B[0mto\u001B[0m \u001B[0mmodel\u001B[0m \u001B[0mtraining\u001B[0m\u001B[0;34m.\u001B[0m \u001B[0mCan\u001B[0m \u001B[0mbe\u001B[0m \u001B[0mused\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0ma\u001B[0m \u001B[0mpipeline\u001B[0m \u001B[0mto\u001B[0m \u001B[0mpredict\u001B[0m \u001B[0mon\u001B[0m \u001B[0mnew\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: Cannot create a consistent method resolution\norder (MRO) for bases ClassificationModels, PreprocessingBluePrint"
     ]
    }
   ],
   "source": [
    "# load libraries\n",
    "from e2eml.classification import classification_blueprints as cb\n",
    "from e2eml.full_processing.postprocessing import save_to_production, load_for_production\n",
    "from e2eml.test.classification_blueprints_test import steel_fault_multiclass_data\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "Load & preprocess steel faults dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load steel faults data\n",
    "test_df, test_target, val_df, val_df_target, test_categorical_cols = steel_fault_multiclass_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using e2eml - Run and save a pipeline\n",
    "In this notebook we configure a custom pipeline. Due to the object-oriented approach we can easily set this up.\n",
    "Under the hood the main and mostly used blueprint pipeline looks like this:\n",
    "\n",
    "def pipeline(self):\n",
    "        logging.info('Start blueprint.')\n",
    "        try:\n",
    "            if df.empty:\n",
    "                skip_train = False\n",
    "            else:\n",
    "                self.dataframe = df\n",
    "                skip_train = True\n",
    "        except AttributeError:\n",
    "            skip_train = False\n",
    "        self.train_test_split(how=self.train_split_type)\n",
    "        self.datetime_converter(datetime_handling='all', force_conversion=False)\n",
    "        if preprocessing_type == 'nlp':\n",
    "            self.pos_tagging_pca()\n",
    "        self.rare_feature_processor(threshold=0.03, mask_as='miscellaneous')\n",
    "        self.cardinality_remover(threshold=100)\n",
    "        self.onehot_pca()\n",
    "        self.category_encoding(algorithm='target')\n",
    "        self.delete_high_null_cols(threshold=0.5)\n",
    "        self.fill_nulls(how='static')\n",
    "        self.data_binning(nb_bins=10)\n",
    "        #self.skewness_removal()\n",
    "        self.outlier_care(method='isolation', how='append')\n",
    "        self.remove_collinearity(threshold=0.8)\n",
    "        self.clustering_as_a_feature(algorithm='dbscan', eps=0.3, n_jobs=-1, min_samples=10)\n",
    "        for nb_cluster in range(2, 10):\n",
    "            self.clustering_as_a_feature(algorithm='kmeans', nb_clusters=nb_cluster)\n",
    "        if self.low_memory_mode:\n",
    "            self.reduce_memory_footprint()\n",
    "        self.automated_feature_selection(metric='logloss')\n",
    "        self.sort_columns_alphabetically()\n",
    "        if skip_train:\n",
    "            pass\n",
    "        else:\n",
    "            self.lgbm_train(tune_mode=self.tune_mode)\n",
    "        self.lgbm_predict(feat_importance=True)\n",
    "        self.classification_eval('lgbm')\n",
    "        self.prediction_mode = True\n",
    "        logging.info('Finished blueprint.')\n",
    "\n",
    "From here we can make custom choices by:\n",
    "- skipping steps\n",
    "- changing parameters\n",
    "- or even extend\n",
    "\n",
    "We follow these steps:\n",
    "- instantiate class\n",
    "- define and run pipeline\n",
    "- save and load pipeline\n",
    "- predict on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate class\n",
    "steel_faults_ml = cb.ClassificationBluePrint(datasource=test_df,\n",
    "                                       target_variable=test_target,\n",
    "                                       categorical_columns=test_categorical_cols,\n",
    "                                       preferred_training_mode='auto',\n",
    "                                       tune_mode='accurate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define custom pipeline...\n",
    "- Please note, that there are logical and technical dependencies. Not everything is possible.\n",
    "\"\"\"\n",
    "def custom_pipeline(df, steel_faults_ml):\n",
    "    try:\n",
    "        if df.empty:\n",
    "            steel_faults_ml.prediction_mode = False\n",
    "        else:\n",
    "            steel_faults_ml.dataframe = df\n",
    "            steel_faults_ml.prediction_mode = True\n",
    "    except AttributeError:\n",
    "        steel_faults_ml.prediction_mode = False\n",
    "    steel_faults_ml.train_test_split(how=steel_faults_ml.train_split_type)\n",
    "    try:\n",
    "        print(steel_faults_ml.df_dict[\"Y_train\"])\n",
    "    except AttributeError:\n",
    "        #does not exist in prediction mode\n",
    "        pass\n",
    "    steel_faults_ml.datetime_converter(datetime_handling='all', force_conversion=False)\n",
    "    steel_faults_ml.pos_tagging_pca()\n",
    "    # we removed rare feature processing\n",
    "    steel_faults_ml.cardinality_remover(threshold=200) #raised\n",
    "    steel_faults_ml.onehot_pca()\n",
    "    steel_faults_ml.category_encoding(algorithm='target')\n",
    "    \"\"\"\n",
    "    Custom pipelines allow you to inject your own data manipulation or to access the data in between.\n",
    "    The train and test data is always stored in the df_dict attribute.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(steel_faults_ml.df_dict[\"X_train\"].isna().sum())\n",
    "    except AttributeError:\n",
    "        #changed location in prediction mode\n",
    "        print(steel_faults_ml.dataframe.isna().sum())\n",
    "    steel_faults_ml.delete_high_null_cols(threshold=0.4) # lowered to 40%\n",
    "    steel_faults_ml.fill_nulls(how='iterative_imputation') # we changed to iterative filling instead of imputation with 0\n",
    "    steel_faults_ml.data_binning(nb_bins=5) # we change the bins\n",
    "    steel_faults_ml.outlier_care(method='isolation', how='append')\n",
    "    steel_faults_ml.remove_collinearity(threshold=0.8)\n",
    "    steel_faults_ml.clustering_as_a_feature(algorithm='dbscan', eps=0.3, n_jobs=-1, min_samples=10)\n",
    "    for nb_cluster in range(2, 20):\n",
    "        steel_faults_ml.clustering_as_a_feature(algorithm='GLMM', nb_clusters=nb_cluster) #changed from kmeans\n",
    "    steel_faults_ml.automated_feature_selection(metric='logloss') # needs to be xgboost compatible\n",
    "    steel_faults_ml.sort_columns_alphabetically()\n",
    "    if steel_faults_ml.prediction_mode:\n",
    "        pass\n",
    "    else:\n",
    "        steel_faults_ml.lgbm_train(tune_mode=steel_faults_ml.tune_mode)\n",
    "    steel_faults_ml.lgbm_predict(feat_importance=True)\n",
    "    steel_faults_ml.classification_eval('lgbm')\n",
    "    steel_faults_ml.prediction_mode = True # mandatory\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run custom blueprint\n",
    "custom_pipeline(None, steel_faults_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pipeline\n",
    "try:\n",
    "    save_to_production(steel_faults_ml, file_name='steel_faults_instance')\n",
    "except AttributeError:\n",
    "    print(\"\"\"Unfortunately this does not work when e2eml has to label encode the target labels automatically.\n",
    "    For saving a pipeline please provide encoded target labels. As of now this is just a fallback solution.\n",
    "    This might be solved in future releases.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on new data\n",
    "In the beginning we kept a holdout dataset. We use this to simulate prediction on completely new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stored pipeline...we skip this accordingly\n",
    "steel_faults_ml_loaded = load_for_production(file_name='steel_faults_instance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encode targets\n",
    "val_df_target = steel_faults_ml_loaded.label_encoder_decoder(val_df_target, mode='transform')\n",
    "\n",
    "# predict on new data\n",
    "custom_pipeline(val_df, steel_faults_ml_loaded)\n",
    "\n",
    "# access predicted labels\n",
    "val_y_hat = steel_faults_ml_loaded.predicted_classes['lgbm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess prediction quality on holdout data\n",
    "print(classification_report(val_df_target, val_y_hat))\n",
    "try:\n",
    "    matthews = matthews_corrcoef(val_df_target, val_y_hat)\n",
    "except Exception:\n",
    "    print(\"Matthew failed.\")\n",
    "    matthews = 0\n",
    "print(matthews)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}