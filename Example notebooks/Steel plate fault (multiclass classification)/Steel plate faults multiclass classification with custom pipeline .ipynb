{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Steel plate faults multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "from e2eml.classification import classification_blueprints as cb\n",
    "from e2eml.full_processing.postprocessing import save_to_production, load_for_production\n",
    "from e2eml.test.classification_blueprints_test import steel_fault_multiclass_data\n",
    "import pandas as pd\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import classification_report\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "Load & preprocess steel faults dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load steel faults data\n",
    "test_df, test_target, val_df, val_df_target, test_categorical_cols = steel_fault_multiclass_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using e2eml - Run and save a pipeline\n",
    "In this notebook we configure a custom pipeline. Due to the object oriented approach we can easily set this up.\n",
    "Under the hood the main and mostly used blueprint pipeline looks like this:\n",
    "\n",
    "def pipeline(self):\n",
    "        logging.info('Start blueprint.')\n",
    "        try:\n",
    "            if df.empty:\n",
    "                skip_train = False\n",
    "            else:\n",
    "                self.dataframe = df\n",
    "                skip_train = True\n",
    "        except AttributeError:\n",
    "            skip_train = False\n",
    "        self.train_test_split(how=self.train_split_type)\n",
    "        self.datetime_converter(datetime_handling='all', force_conversion=False)\n",
    "        if preprocessing_type == 'nlp':\n",
    "            self.pos_tagging_pca()\n",
    "        self.rare_feature_processor(threshold=0.03, mask_as='miscellaneous')\n",
    "        self.cardinality_remover(threshold=100)\n",
    "        self.onehot_pca()\n",
    "        self.category_encoding(algorithm='target')\n",
    "        self.delete_high_null_cols(threshold=0.5)\n",
    "        self.fill_nulls(how='static')\n",
    "        self.data_binning(nb_bins=10)\n",
    "        #self.skewness_removal()\n",
    "        self.outlier_care(method='isolation', how='append')\n",
    "        self.remove_collinearity(threshold=0.8)\n",
    "        self.clustering_as_a_feature(algorithm='dbscan', eps=0.3, n_jobs=-1, min_samples=10)\n",
    "        for nb_cluster in range(2, 10):\n",
    "            self.clustering_as_a_feature(algorithm='kmeans', nb_clusters=nb_cluster)\n",
    "        if self.low_memory_mode:\n",
    "            self.reduce_memory_footprint()\n",
    "        self.automated_feature_selection(metric='logloss')\n",
    "        self.sort_columns_alphabetically()\n",
    "        if skip_train:\n",
    "            pass\n",
    "        else:\n",
    "            self.lgbm_train(tune_mode=self.tune_mode)\n",
    "        self.lgbm_predict(feat_importance=True)\n",
    "        self.classification_eval('lgbm')\n",
    "        self.prediction_mode = True\n",
    "        logging.info('Finished blueprint.')\n",
    "\n",
    "From here we can make custom choices by:\n",
    "- skipping steps\n",
    "- changing parameters\n",
    "- or even extend\n",
    "\n",
    "We follow these steps:\n",
    "- instantiate class\n",
    "- define and run pipeline\n",
    "- save and load pipeline\n",
    "- predict on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preferred training mode auto has been chosen. e2eml will automatically detect, if LGBM and Xgboost canuse GPU acceleration and optimize the workflow accordingly.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate class\n",
    "steel_faults_ml = cb.ClassificationBluePrint(datasource=test_df,\n",
    "                                       target_variable=test_target,\n",
    "                                       categorical_columns=test_categorical_cols,\n",
    "                                       preferred_training_mode='auto',\n",
    "                                       tune_mode='accurate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define custom pipeline...\n",
    "- Please note, that there are logical and technical dependencies. Not everything is possible.\n",
    "\"\"\"\n",
    "def custom_pipeline(df):\n",
    "    try:\n",
    "        if df.empty:\n",
    "            skip_train = False\n",
    "        else:\n",
    "            steel_faults_ml.dataframe = df\n",
    "            skip_train = True\n",
    "    except AttributeError:\n",
    "        skip_train = False\n",
    "    steel_faults_ml.train_test_split(how=steel_faults_ml.train_split_type)\n",
    "    steel_faults_ml.datetime_converter(datetime_handling='all', force_conversion=False)\n",
    "    steel_faults_ml.pos_tagging_pca()\n",
    "    # we removed rare feature processing\n",
    "    steel_faults_ml.cardinality_remover(threshold=100)\n",
    "    steel_faults_ml.onehot_pca()\n",
    "    steel_faults_ml.category_encoding(algorithm='target')\n",
    "    steel_faults_ml.delete_high_null_cols(threshold=0.5)\n",
    "    steel_faults_ml.fill_nulls(how='iterative') # we changed to iterative filling\n",
    "    steel_faults_ml.data_binning(nb_bins=20) # we change the bins\n",
    "    steel_faults_ml.outlier_care(method='isolation', how='append')\n",
    "    steel_faults_ml.remove_collinearity(threshold=0.8)\n",
    "    steel_faults_ml.clustering_as_a_feature(algorithm='GLMM', eps=0.3, n_jobs=-1, min_samples=10)\n",
    "    for nb_cluster in range(2, 20):\n",
    "        steel_faults_ml.clustering_as_a_feature(algorithm='kmeans', nb_clusters=nb_cluster)\n",
    "    steel_faults_ml.automated_feature_selection(metric='mlogloss') # needs to be xgboost compatible\n",
    "    steel_faults_ml.sort_columns_alphabetically()\n",
    "    if skip_train:\n",
    "        pass\n",
    "    else:\n",
    "        steel_faults_ml.lgbm_train(tune_mode=steel_faults_ml.tune_mode)\n",
    "    steel_faults_ml.lgbm_predict(feat_importance=True)\n",
    "    steel_faults_ml.classification_eval('lgbm')\n",
    "    steel_faults_ml.prediction_mode = True # mandatory\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Execute test train split at 21:33:27.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ClassificationBluePrint' object has no attribute 'dataframe'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-13-7c8207e456c8>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Run custom blueprint\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mcustom_pipeline\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-12-b86740c632bb>\u001B[0m in \u001B[0;36mcustom_pipeline\u001B[0;34m(df)\u001B[0m\n\u001B[1;32m     13\u001B[0m         \u001B[0mskip_train\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m     \u001B[0msteel_faults_ml\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain_test_split\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhow\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0msteel_faults_ml\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain_split_type\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 15\u001B[0;31m     \u001B[0msteel_faults_ml\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdatetime_converter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdatetime_handling\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'all'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mforce_conversion\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     16\u001B[0m     \u001B[0msteel_faults_ml\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpos_tagging_pca\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m     \u001B[0;31m# we removed rare feature processing\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/IdeaProjects/e2e_ml/e2eml/full_processing/cpu_preprocessing.py\u001B[0m in \u001B[0;36mdatetime_converter\u001B[0;34m(self, datetime_handling, force_conversion)\u001B[0m\n\u001B[1;32m   1051\u001B[0m                 \u001B[0mdate_columns\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1052\u001B[0m                 \u001B[0;31m# convert date columns from object to datetime type\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1053\u001B[0;31m                 \u001B[0;32mfor\u001B[0m \u001B[0mcol\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataframe\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1054\u001B[0m                     \u001B[0;32mif\u001B[0m \u001B[0mcol\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnum_columns\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1055\u001B[0m                         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'ClassificationBluePrint' object has no attribute 'dataframe'"
     ]
    }
   ],
   "source": [
    "# Run custom blueprint\n",
    "custom_pipeline(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'PreProcessing.label_encoder_decoder.<locals>.PandasLabelEncoder'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-11-e79d1e7683e2>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Save pipeline\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0msave_to_production\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msteel_faults_ml\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfile_name\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'steel_faults_instance'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/IdeaProjects/e2e_ml/e2eml/full_processing/postprocessing.py\u001B[0m in \u001B[0;36msave_to_production\u001B[0;34m(class_instance, file_path, file_name, file_type, clean)\u001B[0m\n\u001B[1;32m     32\u001B[0m         \u001B[0mfull_path\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfile_name\u001B[0m\u001B[0;34m+\u001B[0m\u001B[0mfile_type\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     33\u001B[0m     \u001B[0mfilehandler\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfull_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'wb'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 34\u001B[0;31m     \u001B[0mpickle\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdump\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mclass_instance\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfilehandler\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     35\u001B[0m     \u001B[0mfilehandler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     36\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: Can't pickle local object 'PreProcessing.label_encoder_decoder.<locals>.PandasLabelEncoder'"
     ]
    }
   ],
   "source": [
    "# Save pipeline\n",
    "save_to_production(steel_faults_ml, file_name='steel_faults_instance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on new data\n",
    "In the beginning we kept a holdout dataset. We use this to simulate prediction on completely new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stored pipeline\n",
    "steel_faults_ml_loaded = load_for_production(file_name='steel_faults_instance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on new data\n",
    "custom_pipeline(val_df)\n",
    "\n",
    "# access predicted labels\n",
    "val_y_hat = steel_faults_ml_loaded.predicted_classes['lgbm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess prediction quality on holdout data\n",
    "print(classification_report(val_df_target, val_y_hat))\n",
    "try:\n",
    "    matthews = matthews_corrcoef(val_df_target, val_y_hat)\n",
    "except Exception:\n",
    "    print(\"Matthew failed.\")\n",
    "    matthews = 0\n",
    "print(matthews)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}